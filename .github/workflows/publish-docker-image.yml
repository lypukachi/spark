name: Publish docker image

on: [pull_request]
#on:
#  release:
#    types: [created]

jobs:
  publish-release:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout latest code
      uses: actions/checkout@v2
    - name: Get the version
      id: get_version
      run: echo ::set-output name=VERSION::${GITHUB_REF/refs\/tags\//}
      # access it through ${{ steps.get_version.outputs.VERSION }}
    - name: Set up JDK 8
      uses: actions/setup-java@v1
      with:
        java-version: 8
    - uses: r-lib/actions/setup-r@v1
      with:
        r-version: '3.6.2'
    - name: install R packages
      run: |
        sudo apt-get install -y texlive-latex-base texlive texlive-fonts-extra texinfo qpdf
        sudo Rscript -e "install.packages(c('curl', 'xml2', 'httr', 'devtools', 'testthat', 'knitr', 'rmarkdown', 'roxygen2', 'e1071', 'survival'), repos='https://cloud.r-project.org/')"
      env:
        DEBIAN_FRONTEND: noninteractive
        DEBCONF_NONINTERACTIVE_SEEN: true
    - uses: actions/setup-python@v1
      with:
        python-version: '3.x'
        architecture: 'x64'
    - name: Apply Patch
      run: |-
        # Remove -s option of tini. while gvisor does not support PR_SET_CHILD_SUBREAPER
        sed -i 's/tini -s/tini/' resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh

        # take Dockerfile before WORKDIR starts
        awk '!p;/WORKDIR/{p=1}' resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile | grep -v WORKDIR >> /tmp/Dockerfile

        # try add pyspark before apt-get install
        echo "RUN mkdir /opt/spark/python" >> /tmp/Dockerfile
        echo "COPY python/pyspark /opt/spark/python/pyspark" >> /tmp/Dockerfile

        # and then append pyspark, sparkR requirement
        awk '/RUN mkdir/,/COPY python\/lib/' resource-managers/kubernetes/docker/src/main/dockerfiles/spark/bindings/python/Dockerfile >> /tmp/Dockerfile
        awk '/RUN mkdir/,/ENV R_HOME/' resource-managers/kubernetes/docker/src/main/dockerfiles/spark/bindings/R/Dockerfile >> /tmp/Dockerfile

        # append Dockerfile from WORKDIR to ENTRYPOINT
        awk '/WORKDIR/{p=1}/ENTRYPOINT/{print;p=0}p' resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile >> /tmp/Dockerfile

        # Add passwd entry. otherwise, entrypoint.sh will shows 'Container ENTRYPOINT failed to add passwd entry for anonymous UID'
        # and executor will fail with  javax.security.auth.login.LoginException: java.lang.NullPointerException: invalid null input: name at com.sun.security.auth.UnixPrincipal.<init>(UnixPrincipal.java:71)
        echo 'RUN groupadd --gid $spark_uid spark && useradd -ms /bin/bash spark --uid $spark_uid --gid $spark_uid && chown -R spark:spark /opt/spark/work-dir' >> /tmp/Dockerfile
        echo 'USER ${spark_uid}' >> /tmp/Dockerfile

        # patch 'mkdir' -> 'mkdir -p'
        cat /tmp/Dockerfile | \
        sed 's:RUN mkdir ${SPARK_HOME}/python:RUN mkdir -p ${SPARK_HOME}/python:g' | \
        sed 's:RUN mkdir ${SPARK_HOME}/R:RUN mkdir -p ${SPARK_HOME}/R:g' > resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile

        # display
        cat resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile
    - name: Build distribution
      run: |-
        ./dev/make-distribution.sh --name spark --pip --r --tgz -Psparkr -Phadoop-2.7 -Phive -Phive-thriftserver -Pkubernetes
      env:
        DEBIAN_FRONTEND: noninteractive
        DEBCONF_NONINTERACTIVE_SEEN: true
    - name: Print dir
      run: |-
        ls -al .
        ls -al python
        ls -al python/pyspark
        ls -al R
        ls -al dist
        ls -al dist/python
        ls -al dist/python/pyspark
        ls -al dist/R
        ls -al dist/kubernetes
        ls -al dist/kubernetes/dockerfiles
        ls -al dist/kubernetes/dockerfiles/spark
    - name: Build docker image
      working-directory: ./dist
      run: |-
        # export SPARK_HOME=`pwd`/dist
        # ./bin/docker-image-tool.sh -r opendatastudio -t ${{ steps.get_version.outputs.VERSION }} build
        ./bin/docker-image-tool.sh -r opendatastudio -t test build
    - name: Push image
      run: |-
        docker images
        #echo "${{ secrets.DOCKER_PASSWORD }}" | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin
        #docker push opendatastudio/spark:${{ steps.get_version.outputs.VERSION }}
